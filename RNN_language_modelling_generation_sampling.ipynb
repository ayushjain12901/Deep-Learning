{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_language-modelling_generation_sampling",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWAutjzBJGP_"
      },
      "source": [
        "**Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJSElkWWILqu"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Embedding, SimpleRNN,Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "import numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYpWPT4OJJjI"
      },
      "source": [
        "**Probability calculation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYPVW7WLG-Pf"
      },
      "source": [
        "def prob(model,tok,sentence):\n",
        "    print(\"Input Sentence is: \",sentence)\n",
        "    encoded=tok.texts_to_sequences([sentence])[0]\n",
        "    encoded.insert(0,0)\n",
        "    encoded=array(encoded)\n",
        "    encoded=numpy.reshape(encoded,newshape=(1,-1))\n",
        "    print(\"encoded: \",encoded,encoded.shape)\n",
        "    prob=model.predict_proba(encoded,verbose=0)\n",
        "    #print(\"Prob: \",prob.shape) #(1,words,23)\n",
        "    probability=1\n",
        "    for i in range(prob.shape[1]-1):\n",
        "        probability=probability*prob[0,i,encoded[0,i+1]]\n",
        "    print(\"Probability of sentence is : \",probability)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFApxuX1JOvZ"
      },
      "source": [
        "**Preparing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur_qVvRDH9fS",
        "outputId": "a78ec200-33fe-4690-b314-6cad6e37c0c5"
      },
      "source": [
        "data=['Jack and Jill went up the hill .','To fetch a pail of water .','Jack fell down and broke his crown .',\n",
        "      'And Jill came tumbling after .']\n",
        "\n",
        "tok= Tokenizer(filters='~`!@#$%^&*()-_+={}[]:;<\\>/?|,')\n",
        "\n",
        "tok.fit_on_texts(data)# learns the vocab\n",
        "print(\"Word indices: \",tok.word_index)\n",
        "\n",
        "vocab_size= len(tok.word_index)+1\n",
        "print('Vocab Size is: ',vocab_size)\n",
        "\n",
        "Num_sequences= tok.texts_to_sequences(data)\n",
        "print(\"Sequences: \",Num_sequences,len(Num_sequences))\n",
        "\n",
        "X=[]\n",
        "Y=[]\n",
        "for i in range(len(Num_sequences)):\n",
        "    X.insert(i,Num_sequences[i][:-1])\n",
        "    Y.insert(i,Num_sequences[i])\n",
        "\n",
        "maxl=max([len(i) for i in X])\n",
        "\n",
        "X=pad_sequences(X,maxlen=maxl+1,padding='pre')\n",
        "print(\"After padding X: {}\\n \".format(X))\n",
        "Y=pad_sequences(Y,maxlen=maxl+1,padding='pre')\n",
        "print(\"After padding Y: {}\\n\".format(Y))\n",
        "\n",
        "Y=to_categorical(Y,num_classes=vocab_size)\n",
        "\n",
        "print(\"X=\",X,\"\\nY=\",Y,X.shape,Y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word indices:  {'.': 1, 'and': 2, 'jack': 3, 'jill': 4, 'went': 5, 'up': 6, 'the': 7, 'hill': 8, 'to': 9, 'fetch': 10, 'a': 11, 'pail': 12, 'of': 13, 'water': 14, 'fell': 15, 'down': 16, 'broke': 17, 'his': 18, 'crown': 19, 'came': 20, 'tumbling': 21, 'after': 22}\n",
            "Vocab Size is:  23\n",
            "Sequences:  [[3, 2, 4, 5, 6, 7, 8, 1], [9, 10, 11, 12, 13, 14, 1], [3, 15, 16, 2, 17, 18, 19, 1], [2, 4, 20, 21, 22, 1]] 4\n",
            "After padding X: [[ 0  3  2  4  5  6  7  8]\n",
            " [ 0  0  9 10 11 12 13 14]\n",
            " [ 0  3 15 16  2 17 18 19]\n",
            " [ 0  0  0  2  4 20 21 22]]\n",
            " \n",
            "After padding Y: [[ 3  2  4  5  6  7  8  1]\n",
            " [ 0  9 10 11 12 13 14  1]\n",
            " [ 3 15 16  2 17 18 19  1]\n",
            " [ 0  0  2  4 20 21 22  1]]\n",
            "\n",
            "X= [[ 0  3  2  4  5  6  7  8]\n",
            " [ 0  0  9 10 11 12 13 14]\n",
            " [ 0  3 15 16  2 17 18 19]\n",
            " [ 0  0  0  2  4 20 21 22]] \n",
            "Y= [[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]] (4, 8) (4, 8, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f99Br8JJUvw"
      },
      "source": [
        "**Sequential Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3A_AExHtIZ9d",
        "outputId": "fdd7f212-bd20-4770-db9b-e6f78ef6d8d6"
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=10))\n",
        "model.add(SimpleRNN(units=50, return_sequences=True))# 50 hidden neurons\n",
        "model.add(Dense(units=vocab_size,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
        "model.fit(X,Y,epochs=250,verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.1208 - accuracy: 0.1250\n",
            "Epoch 2/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.0888 - accuracy: 0.1875\n",
            "Epoch 3/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.0604 - accuracy: 0.2188\n",
            "Epoch 4/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.0290 - accuracy: 0.2500\n",
            "Epoch 5/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.9928 - accuracy: 0.1875\n",
            "Epoch 6/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.9516 - accuracy: 0.1250\n",
            "Epoch 7/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.9069 - accuracy: 0.1250\n",
            "Epoch 8/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.8607 - accuracy: 0.1250\n",
            "Epoch 9/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.8146 - accuracy: 0.1250\n",
            "Epoch 10/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.7689 - accuracy: 0.1250\n",
            "Epoch 11/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7232 - accuracy: 0.1562\n",
            "Epoch 12/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6773 - accuracy: 0.2188\n",
            "Epoch 13/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6311 - accuracy: 0.2500\n",
            "Epoch 14/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5851 - accuracy: 0.3125\n",
            "Epoch 15/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5398 - accuracy: 0.3125\n",
            "Epoch 16/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.4958 - accuracy: 0.3125\n",
            "Epoch 17/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.4532 - accuracy: 0.3125\n",
            "Epoch 18/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4123 - accuracy: 0.4062\n",
            "Epoch 19/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.3729 - accuracy: 0.4375\n",
            "Epoch 20/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3349 - accuracy: 0.4688\n",
            "Epoch 21/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2982 - accuracy: 0.4375\n",
            "Epoch 22/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2624 - accuracy: 0.4375\n",
            "Epoch 23/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.2276 - accuracy: 0.4375\n",
            "Epoch 24/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.1937 - accuracy: 0.4375\n",
            "Epoch 25/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.1605 - accuracy: 0.4375\n",
            "Epoch 26/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.1280 - accuracy: 0.4375\n",
            "Epoch 27/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0962 - accuracy: 0.4688\n",
            "Epoch 28/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.0651 - accuracy: 0.5625\n",
            "Epoch 29/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0346 - accuracy: 0.5625\n",
            "Epoch 30/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.0048 - accuracy: 0.5625\n",
            "Epoch 31/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9756 - accuracy: 0.5625\n",
            "Epoch 32/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9471 - accuracy: 0.5938\n",
            "Epoch 33/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9192 - accuracy: 0.5938\n",
            "Epoch 34/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.8919 - accuracy: 0.5938\n",
            "Epoch 35/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8651 - accuracy: 0.6250\n",
            "Epoch 36/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8389 - accuracy: 0.6250\n",
            "Epoch 37/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8132 - accuracy: 0.6250\n",
            "Epoch 38/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7879 - accuracy: 0.6250\n",
            "Epoch 39/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7632 - accuracy: 0.6250\n",
            "Epoch 40/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7389 - accuracy: 0.6562\n",
            "Epoch 41/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.7150 - accuracy: 0.6562\n",
            "Epoch 42/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6915 - accuracy: 0.6562\n",
            "Epoch 43/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6685 - accuracy: 0.6562\n",
            "Epoch 44/250\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.6458 - accuracy: 0.6875\n",
            "Epoch 45/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.6234 - accuracy: 0.6875\n",
            "Epoch 46/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6014 - accuracy: 0.6875\n",
            "Epoch 47/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5798 - accuracy: 0.6875\n",
            "Epoch 48/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.5585 - accuracy: 0.6875\n",
            "Epoch 49/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5375 - accuracy: 0.6875\n",
            "Epoch 50/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.5168 - accuracy: 0.7188\n",
            "Epoch 51/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4964 - accuracy: 0.7188\n",
            "Epoch 52/250\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4763 - accuracy: 0.7188\n",
            "Epoch 53/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4565 - accuracy: 0.7188\n",
            "Epoch 54/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4369 - accuracy: 0.7500\n",
            "Epoch 55/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.4176 - accuracy: 0.7500\n",
            "Epoch 56/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3986 - accuracy: 0.7500\n",
            "Epoch 57/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3798 - accuracy: 0.7500\n",
            "Epoch 58/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3612 - accuracy: 0.7500\n",
            "Epoch 59/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3429 - accuracy: 0.7500\n",
            "Epoch 60/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3248 - accuracy: 0.7500\n",
            "Epoch 61/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3069 - accuracy: 0.7812\n",
            "Epoch 62/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2892 - accuracy: 0.7812\n",
            "Epoch 63/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2717 - accuracy: 0.7812\n",
            "Epoch 64/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2545 - accuracy: 0.7812\n",
            "Epoch 65/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2379 - accuracy: 0.7812\n",
            "Epoch 66/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.2226 - accuracy: 0.7812\n",
            "Epoch 67/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2080 - accuracy: 0.7812\n",
            "Epoch 68/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1917 - accuracy: 0.7812\n",
            "Epoch 69/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1751 - accuracy: 0.7812\n",
            "Epoch 70/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1588 - accuracy: 0.7812\n",
            "Epoch 71/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1432 - accuracy: 0.7812\n",
            "Epoch 72/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.1279 - accuracy: 0.7812\n",
            "Epoch 73/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1129 - accuracy: 0.7812\n",
            "Epoch 74/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0980 - accuracy: 0.7812\n",
            "Epoch 75/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0834 - accuracy: 0.7812\n",
            "Epoch 76/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0689 - accuracy: 0.7812\n",
            "Epoch 77/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0546 - accuracy: 0.7812\n",
            "Epoch 78/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0406 - accuracy: 0.7812\n",
            "Epoch 79/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0267 - accuracy: 0.7812\n",
            "Epoch 80/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0132 - accuracy: 0.7812\n",
            "Epoch 81/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.9998 - accuracy: 0.8125\n",
            "Epoch 82/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9866 - accuracy: 0.7812\n",
            "Epoch 83/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9734 - accuracy: 0.8125\n",
            "Epoch 84/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9602 - accuracy: 0.8438\n",
            "Epoch 85/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9471 - accuracy: 0.8438\n",
            "Epoch 86/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.9341 - accuracy: 0.8438\n",
            "Epoch 87/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9213 - accuracy: 0.8438\n",
            "Epoch 88/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9087 - accuracy: 0.8438\n",
            "Epoch 89/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8962 - accuracy: 0.8438\n",
            "Epoch 90/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8839 - accuracy: 0.8438\n",
            "Epoch 91/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.8718 - accuracy: 0.8438\n",
            "Epoch 92/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.8598 - accuracy: 0.8438\n",
            "Epoch 93/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8480 - accuracy: 0.8438\n",
            "Epoch 94/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8363 - accuracy: 0.8438\n",
            "Epoch 95/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.8248 - accuracy: 0.8438\n",
            "Epoch 96/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.8134 - accuracy: 0.8438\n",
            "Epoch 97/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8021 - accuracy: 0.8438\n",
            "Epoch 98/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7910 - accuracy: 0.8438\n",
            "Epoch 99/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7800 - accuracy: 0.8438\n",
            "Epoch 100/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7690 - accuracy: 0.8438\n",
            "Epoch 101/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7581 - accuracy: 0.8438\n",
            "Epoch 102/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7474 - accuracy: 0.8438\n",
            "Epoch 103/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7367 - accuracy: 0.8438\n",
            "Epoch 104/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7262 - accuracy: 0.8438\n",
            "Epoch 105/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7158 - accuracy: 0.8438\n",
            "Epoch 106/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7056 - accuracy: 0.8438\n",
            "Epoch 107/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6955 - accuracy: 0.8438\n",
            "Epoch 108/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6855 - accuracy: 0.8438\n",
            "Epoch 109/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6756 - accuracy: 0.8438\n",
            "Epoch 110/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6659 - accuracy: 0.8438\n",
            "Epoch 111/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6563 - accuracy: 0.8438\n",
            "Epoch 112/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6468 - accuracy: 0.8438\n",
            "Epoch 113/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6375 - accuracy: 0.8438\n",
            "Epoch 114/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6283 - accuracy: 0.8438\n",
            "Epoch 115/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6192 - accuracy: 0.8438\n",
            "Epoch 116/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6102 - accuracy: 0.8438\n",
            "Epoch 117/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6013 - accuracy: 0.8438\n",
            "Epoch 118/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5925 - accuracy: 0.8438\n",
            "Epoch 119/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5839 - accuracy: 0.8438\n",
            "Epoch 120/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5753 - accuracy: 0.8438\n",
            "Epoch 121/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5669 - accuracy: 0.8438\n",
            "Epoch 122/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5587 - accuracy: 0.8438\n",
            "Epoch 123/250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5505 - accuracy: 0.8438\n",
            "Epoch 124/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5425 - accuracy: 0.8438\n",
            "Epoch 125/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5347 - accuracy: 0.8438\n",
            "Epoch 126/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5270 - accuracy: 0.8438\n",
            "Epoch 127/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5194 - accuracy: 0.8438\n",
            "Epoch 128/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5120 - accuracy: 0.8750\n",
            "Epoch 129/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5047 - accuracy: 0.8438\n",
            "Epoch 130/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4975 - accuracy: 0.8750\n",
            "Epoch 131/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4904 - accuracy: 0.8750\n",
            "Epoch 132/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4833 - accuracy: 0.8750\n",
            "Epoch 133/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4764 - accuracy: 0.8750\n",
            "Epoch 134/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4696 - accuracy: 0.8750\n",
            "Epoch 135/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4629 - accuracy: 0.8750\n",
            "Epoch 136/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4564 - accuracy: 0.8750\n",
            "Epoch 137/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4500 - accuracy: 0.8750\n",
            "Epoch 138/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4437 - accuracy: 0.8750\n",
            "Epoch 139/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4375 - accuracy: 0.8750\n",
            "Epoch 140/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4315 - accuracy: 0.8750\n",
            "Epoch 141/250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4257 - accuracy: 0.8750\n",
            "Epoch 142/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4200 - accuracy: 0.8750\n",
            "Epoch 143/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4144 - accuracy: 0.8750\n",
            "Epoch 144/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4090 - accuracy: 0.8750\n",
            "Epoch 145/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4037 - accuracy: 0.8750\n",
            "Epoch 146/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3983 - accuracy: 0.8750\n",
            "Epoch 147/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3930 - accuracy: 0.8750\n",
            "Epoch 148/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3878 - accuracy: 0.8750\n",
            "Epoch 149/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3827 - accuracy: 0.8750\n",
            "Epoch 150/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3778 - accuracy: 0.8750\n",
            "Epoch 151/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3730 - accuracy: 0.8750\n",
            "Epoch 152/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3683 - accuracy: 0.8750\n",
            "Epoch 153/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3637 - accuracy: 0.8750\n",
            "Epoch 154/250\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3591 - accuracy: 0.8750\n",
            "Epoch 155/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3547 - accuracy: 0.8750\n",
            "Epoch 156/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3504 - accuracy: 0.8750\n",
            "Epoch 157/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3462 - accuracy: 0.8750\n",
            "Epoch 158/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.3421 - accuracy: 0.8750\n",
            "Epoch 159/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3381 - accuracy: 0.8750\n",
            "Epoch 160/250\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.3341 - accuracy: 0.8750\n",
            "Epoch 161/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3303 - accuracy: 0.8750\n",
            "Epoch 162/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3265 - accuracy: 0.8750\n",
            "Epoch 163/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3227 - accuracy: 0.8750\n",
            "Epoch 164/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3190 - accuracy: 0.8750\n",
            "Epoch 165/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3154 - accuracy: 0.8750\n",
            "Epoch 166/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3118 - accuracy: 0.8750\n",
            "Epoch 167/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3084 - accuracy: 0.8750\n",
            "Epoch 168/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3050 - accuracy: 0.8750\n",
            "Epoch 169/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3017 - accuracy: 0.8750\n",
            "Epoch 170/250\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.2985 - accuracy: 0.8750\n",
            "Epoch 171/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2954 - accuracy: 0.8750\n",
            "Epoch 172/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2923 - accuracy: 0.8750\n",
            "Epoch 173/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2893 - accuracy: 0.8750\n",
            "Epoch 174/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2864 - accuracy: 0.8750\n",
            "Epoch 175/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2835 - accuracy: 0.8750\n",
            "Epoch 176/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2807 - accuracy: 0.8750\n",
            "Epoch 177/250\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.2779 - accuracy: 0.8750\n",
            "Epoch 178/250\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2752 - accuracy: 0.8750\n",
            "Epoch 179/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2725 - accuracy: 0.8750\n",
            "Epoch 180/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2699 - accuracy: 0.8750\n",
            "Epoch 181/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2674 - accuracy: 0.8750\n",
            "Epoch 182/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2649 - accuracy: 0.8750\n",
            "Epoch 183/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2625 - accuracy: 0.8750\n",
            "Epoch 184/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2602 - accuracy: 0.8750\n",
            "Epoch 185/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2579 - accuracy: 0.8750\n",
            "Epoch 186/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2558 - accuracy: 0.8750\n",
            "Epoch 187/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2536 - accuracy: 0.8750\n",
            "Epoch 188/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2515 - accuracy: 0.8750\n",
            "Epoch 189/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2494 - accuracy: 0.8750\n",
            "Epoch 190/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2473 - accuracy: 0.8750\n",
            "Epoch 191/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2453 - accuracy: 0.8750\n",
            "Epoch 192/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2433 - accuracy: 0.8750\n",
            "Epoch 193/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2414 - accuracy: 0.8750\n",
            "Epoch 194/250\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2396 - accuracy: 0.8750\n",
            "Epoch 195/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2378 - accuracy: 0.8750\n",
            "Epoch 196/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2361 - accuracy: 0.8750\n",
            "Epoch 197/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2344 - accuracy: 0.8750\n",
            "Epoch 198/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2327 - accuracy: 0.8750\n",
            "Epoch 199/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2311 - accuracy: 0.8750\n",
            "Epoch 200/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2296 - accuracy: 0.8750\n",
            "Epoch 201/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2281 - accuracy: 0.8750\n",
            "Epoch 202/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2266 - accuracy: 0.8750\n",
            "Epoch 203/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2252 - accuracy: 0.8750\n",
            "Epoch 204/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2238 - accuracy: 0.8750\n",
            "Epoch 205/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2224 - accuracy: 0.8750\n",
            "Epoch 206/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2211 - accuracy: 0.8750\n",
            "Epoch 207/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2198 - accuracy: 0.8750\n",
            "Epoch 208/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2185 - accuracy: 0.8750\n",
            "Epoch 209/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2173 - accuracy: 0.8750\n",
            "Epoch 210/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2161 - accuracy: 0.8750\n",
            "Epoch 211/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2149 - accuracy: 0.8750\n",
            "Epoch 212/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2138 - accuracy: 0.8750\n",
            "Epoch 213/250\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2127 - accuracy: 0.8750\n",
            "Epoch 214/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2116 - accuracy: 0.8750\n",
            "Epoch 215/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2105 - accuracy: 0.8750\n",
            "Epoch 216/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2095 - accuracy: 0.8750\n",
            "Epoch 217/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2085 - accuracy: 0.8750\n",
            "Epoch 218/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.2076 - accuracy: 0.8750\n",
            "Epoch 219/250\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2067 - accuracy: 0.8750\n",
            "Epoch 220/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2058 - accuracy: 0.8750\n",
            "Epoch 221/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2049 - accuracy: 0.8750\n",
            "Epoch 222/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2040 - accuracy: 0.8750\n",
            "Epoch 223/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2032 - accuracy: 0.8750\n",
            "Epoch 224/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2023 - accuracy: 0.8750\n",
            "Epoch 225/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2015 - accuracy: 0.8750\n",
            "Epoch 226/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2008 - accuracy: 0.8750\n",
            "Epoch 227/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2000 - accuracy: 0.8750\n",
            "Epoch 228/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1993 - accuracy: 0.8750\n",
            "Epoch 229/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1986 - accuracy: 0.8750\n",
            "Epoch 230/250\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1980 - accuracy: 0.8750\n",
            "Epoch 231/250\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.1973 - accuracy: 0.8750\n",
            "Epoch 232/250\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.1967 - accuracy: 0.8750\n",
            "Epoch 233/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1961 - accuracy: 0.8750\n",
            "Epoch 234/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1955 - accuracy: 0.8750\n",
            "Epoch 235/250\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1948 - accuracy: 0.8750\n",
            "Epoch 236/250\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1942 - accuracy: 0.8750\n",
            "Epoch 237/250\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1937 - accuracy: 0.8750\n",
            "Epoch 238/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1931 - accuracy: 0.8750\n",
            "Epoch 239/250\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1926 - accuracy: 0.8750\n",
            "Epoch 240/250\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1921 - accuracy: 0.8750\n",
            "Epoch 241/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1917 - accuracy: 0.8750\n",
            "Epoch 242/250\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.1913 - accuracy: 0.8750\n",
            "Epoch 243/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1908 - accuracy: 0.8750\n",
            "Epoch 244/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1903 - accuracy: 0.8750\n",
            "Epoch 245/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1898 - accuracy: 0.8750\n",
            "Epoch 246/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1893 - accuracy: 0.8750\n",
            "Epoch 247/250\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1889 - accuracy: 0.8750\n",
            "Epoch 248/250\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1885 - accuracy: 0.8750\n",
            "Epoch 249/250\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1881 - accuracy: 0.8750\n",
            "Epoch 250/250\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1877 - accuracy: 0.8750\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8b125f0450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcOs2f1GN5Bx"
      },
      "source": [
        "**Language Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rf5L1OO1IaKh",
        "outputId": "d458a367-7c2f-4d26-acd0-3cf2fe88671b"
      },
      "source": [
        "s=input(\"Enter the sentence to find the probabilty: \")\n",
        "prob(model,tok,s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the sentence to find the probabilty: Jack and Jill pail water\n",
            "Input Sentence is:  Jack and Jill pail water\n",
            "encoded:  [[ 0  3  2  4 12 14]] (1, 6)\n",
            "Probability of sentence is :  3.2488325494969366e-10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D497AXEgOFik"
      },
      "source": [
        "**Sentence Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl4n9ySsM_Zb",
        "outputId": "bc14b9ad-a4b5-469b-a117-55e9ea466aee"
      },
      "source": [
        "def generation(n_words):\n",
        "    encoded=[]\n",
        "    in_text=''\n",
        "\n",
        "    for i in range(n_words):\n",
        "        print(\"Intext at starting:\",in_text)\n",
        "        encoded=tok.texts_to_sequences([in_text])[0]\n",
        "        encoded.insert(0,0)\n",
        "        encoded=array(encoded)\n",
        "        encoded=numpy.reshape(encoded,newshape=(1,-1))\n",
        "        print(\"Encoded: \",encoded, encoded.shape)\n",
        "\n",
        "        if i==0:\n",
        "            prob=model.predict_proba(encoded,verbose=0)\n",
        "            yhat=0\n",
        "            while yhat== 0:\n",
        "                yhat=numpy.argmax(prob)\n",
        "            yhat=[yhat]\n",
        "            yhat=array(yhat)\n",
        "            yhat=numpy.reshape(yhat,(1,-1))\n",
        "        else:\n",
        "            prob = model.predict_proba(encoded)\n",
        "            yhat = numpy.append(yhat,0)\n",
        "            yhat = numpy.reshape(yhat, newshape=(1, -1))\n",
        "            while yhat[0,i]==0:\n",
        "                yhat[0,i]=numpy.argmax(prob[0,i])\n",
        "\n",
        "        print(\"yhat: \", yhat)\n",
        "\n",
        "        out_word=''\n",
        "        for word,index in tok.word_index.items():\n",
        "            if index==yhat[0,i]:\n",
        "                out_word=word\n",
        "                break\n",
        "\n",
        "        in_text=in_text+' '+out_word\n",
        "    return in_text\n",
        "\n",
        "\n",
        "l=int(input(\"Enter the length of sentence that you want: \"))\n",
        "s=generation(l)\n",
        "print('Sentence on sampling is: ',s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the length of sentence that you want: 5\n",
            "Intext at starting: \n",
            "Encoded:  [[0]] (1, 1)\n",
            "yhat:  [[3]]\n",
            "Intext at starting:  jack\n",
            "Encoded:  [[0 3]] (1, 2)\n",
            "yhat:  [[ 3 15]]\n",
            "Intext at starting:  jack fell\n",
            "Encoded:  [[ 0  3 15]] (1, 3)\n",
            "yhat:  [[ 3 15 16]]\n",
            "Intext at starting:  jack fell down\n",
            "Encoded:  [[ 0  3 15 16]] (1, 4)\n",
            "yhat:  [[ 3 15 16  2]]\n",
            "Intext at starting:  jack fell down and\n",
            "Encoded:  [[ 0  3 15 16  2]] (1, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "yhat:  [[ 3 15 16  2 17]]\n",
            "Sentence on sampling is:   jack fell down and broke\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YtBThQ3OLbX"
      },
      "source": [
        "**Sentence Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkSoEYNXNb5w",
        "outputId": "57407d97-15d0-4e7f-f9d9-ccc1796fb34f"
      },
      "source": [
        "def sample_without_seed_sampling(n_words):\n",
        "    encoded=[]\n",
        "    in_text=''\n",
        "\n",
        "    for i in range(n_words):\n",
        "        print(\"Intext at starting:\",in_text)\n",
        "        encoded=tok.texts_to_sequences([in_text])[0]\n",
        "        encoded.insert(0,0)\n",
        "        encoded=array(encoded)\n",
        "        encoded=numpy.reshape(encoded,newshape=(1,-1))\n",
        "        print(\"Encoded: \",encoded, encoded.shape)\n",
        "\n",
        "        if i==0:\n",
        "            prob=model.predict_proba(encoded,verbose=0)\n",
        "            yhat=0\n",
        "            while yhat== 0:\n",
        "                yhat=numpy.random.choice(range(vocab_size),p=prob.ravel())\n",
        "            yhat=[yhat]\n",
        "            yhat=array(yhat)\n",
        "            yhat=numpy.reshape(yhat,(1,-1))\n",
        "        else:\n",
        "            prob = model.predict_proba(encoded)\n",
        "            yhat = numpy.append(yhat,0)\n",
        "            yhat = numpy.reshape(yhat, newshape=(1, -1))\n",
        "            while yhat[0,i]==0:\n",
        "                yhat[0,i]=numpy.random.choice(range(vocab_size),p=prob[0,i].ravel())\n",
        "\n",
        "        print(\"yhat: \", yhat)\n",
        "\n",
        "        out_word=''\n",
        "        for word,index in tok.word_index.items():\n",
        "            if index==yhat[0,i]:\n",
        "                out_word=word\n",
        "                break\n",
        "\n",
        "        in_text=in_text+' '+out_word\n",
        "    return in_text\n",
        "\n",
        "l=int(input(\"Enter the length of sentence that you want: \"))\n",
        "s=sample_without_seed_sampling(l)\n",
        "print('Sentence is: ',s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the length of sentence that you want: 5\n",
            "Intext at starting: \n",
            "Encoded:  [[0]] (1, 1)\n",
            "yhat:  [[3]]\n",
            "Intext at starting:  jack\n",
            "Encoded:  [[0 3]] (1, 2)\n",
            "yhat:  [[3 2]]\n",
            "Intext at starting:  jack and\n",
            "Encoded:  [[0 3 2]] (1, 3)\n",
            "yhat:  [[3 2 4]]\n",
            "Intext at starting:  jack and jill\n",
            "Encoded:  [[0 3 2 4]] (1, 4)\n",
            "yhat:  [[3 2 4 5]]\n",
            "Intext at starting:  jack and jill went\n",
            "Encoded:  [[0 3 2 4 5]] (1, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "yhat:  [[3 2 4 5 6]]\n",
            "Sentence is:   jack and jill went up\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}